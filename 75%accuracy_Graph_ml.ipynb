{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPhrvvf3YwJenlAL7y0sayT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akankshaaaa-01/Assignment-Submission-Portal/blob/main/75%25accuracy_Graph_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boTlG286K57e",
        "outputId": "3824c53e-6f41-486b-f900-43c79126c9bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Using device: cpu\n",
            "Using Colab cache for faster access to the 'twibot-20' dataset.\n",
            "\n",
            "📁 Dataset downloaded at: /kaggle/input/twibot-20\n",
            "\n",
            "📊 Dataset sizes:\n",
            "Train: 8278 | Dev: 2365 | Test: 1183\n",
            "\n",
            "🔍 Sample user:\n",
            "{\n",
            "  \"ID\": \"17461978\",\n",
            "  \"profile\": {\n",
            "    \"id\": \"17461978 \",\n",
            "    \"id_str\": \"17461978 \",\n",
            "    \"name\": \"SHAQ \",\n",
            "    \"screen_name\": \"SHAQ \",\n",
            "    \"location\": \"Orlando, FL \",\n",
            "    \"profile_location\": \"{'id': '55b4f9e5c516e0b6', 'url': 'https://api.twitter.com/1.1/geo/id/55b4f9e5c516e0b6.json', 'place_type': 'unknown', 'name': 'Orlando, FL', 'full_name': 'Orlando, FL', 'country_code': '', 'country': '', 'contained_within': [], 'bounding_box': None, 'attributes': {}} \",\n",
            "    \"description\": \"VERY QUOTATIOUS, I PERFORM RANDOM ACTS OF SHAQNESS \",\n",
            "    \"url\": \"https://t.co/7hsiK8cCKW \",\n",
            "    \"entities\": \"{'url': {'urls': [{'url': 'https://t.co/7hsiK8cCKW', 'expanded_url': 'http://www.ShaqFuRadio.com', 'display_url': 'ShaqFuRadio.com', 'indices': [0, 23]}]}, 'description': {'urls': []}} \",\n",
            "    \"protected\": \n"
          ]
        }
      ],
      "source": [
        "# 📦 Install dependencies\n",
        "!pip install torch torch-geometric networkx kagglehub scikit-learn matplotlib seaborn -q\n",
        "\n",
        "import json, numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, networkx as nx\n",
        "import torch, torch.nn.functional as F\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.nn import GCNConv\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
        "import warnings, kagglehub\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"✅ Using device: {device}\")\n",
        "\n",
        "# 🗂️ Download TwiBot-20 dataset\n",
        "path = kagglehub.dataset_download(\"marvinvanbo/twibot-20\")\n",
        "print(\"\\n📁 Dataset downloaded at:\", path)\n",
        "\n",
        "# 🧩 Load JSON files\n",
        "def load_json(path):\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return json.load(f)\n",
        "\n",
        "train_data = load_json(f\"{path}/train.json\")\n",
        "dev_data = load_json(f\"{path}/dev.json\")\n",
        "test_data = load_json(f\"{path}/test.json\")\n",
        "\n",
        "print(f\"\\n📊 Dataset sizes:\")\n",
        "print(f\"Train: {len(train_data)} | Dev: {len(dev_data)} | Test: {len(test_data)}\")\n",
        "print(\"\\n🔍 Sample user:\")\n",
        "print(json.dumps(train_data[0], indent=2)[:800])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 🔧 Enhanced Features for ~81% accuracy\n",
        "def extract_features_v3(user):\n",
        "    profile = user.get('profile', {}) or {}\n",
        "\n",
        "    def safe_num(x, default=0):\n",
        "        try: return float(str(x).strip())\n",
        "        except: return default\n",
        "\n",
        "    # Counts\n",
        "    followers = safe_num(profile.get('followers_count', 0))\n",
        "    friends = safe_num(profile.get('friends_count', 0))\n",
        "    statuses = safe_num(profile.get('statuses_count', 0))\n",
        "    favourites = safe_num(profile.get('favourites_count', 0))\n",
        "    listed = safe_num(profile.get('listed_count', 0))\n",
        "\n",
        "    # Boolean\n",
        "    verified = 1 if str(profile.get('verified')).lower() == 'true' else 0\n",
        "    default_prof = 1 if str(profile.get('default_profile')).lower() == 'true' else 0\n",
        "    default_img = 1 if str(profile.get('default_profile_image')).lower() == 'true' else 0\n",
        "    geo_enabled = 1 if str(profile.get('geo_enabled')).lower() == 'true' else 0\n",
        "\n",
        "    # Critical ratios\n",
        "    fr_ratio = friends / (followers + 1)\n",
        "    fl_ratio = followers / (friends + 1)\n",
        "    tweets_per_day = statuses / max(followers + friends + 1, 1)\n",
        "    favourites_ratio = favourites / (statuses + 1)\n",
        "    listed_ratio = listed / (followers + 1)\n",
        "\n",
        "    # Text info\n",
        "    desc = str(profile.get('description', ''))\n",
        "    name = str(profile.get('name', ''))\n",
        "    screen_name = str(profile.get('screen_name', ''))\n",
        "    has_url = 1 if profile.get('url') and str(profile.get('url')).strip() else 0\n",
        "\n",
        "    # Tweets\n",
        "    tweets = user.get('tweet') or []\n",
        "    num_tweets = len([t for t in tweets if isinstance(t, str)])\n",
        "\n",
        "    if num_tweets > 0:\n",
        "        tweet_lens = [len(str(t)) for t in tweets if isinstance(t, str)]\n",
        "        avg_len = np.mean(tweet_lens)\n",
        "        url_count = sum(str(t).count('http') for t in tweets if isinstance(t, str))\n",
        "        mention_count = sum(str(t).count('@') for t in tweets if isinstance(t, str))\n",
        "        hashtag_count = sum(str(t).count('#') for t in tweets if isinstance(t, str))\n",
        "        rt_count = sum(1 for t in tweets if isinstance(t, str) and str(t).startswith('RT @'))\n",
        "        urls_per_tweet = url_count / num_tweets\n",
        "        mentions_per_tweet = mention_count / num_tweets\n",
        "        hashtags_per_tweet = hashtag_count / num_tweets\n",
        "        retweet_ratio = rt_count / num_tweets\n",
        "    else:\n",
        "        avg_len = urls_per_tweet = mentions_per_tweet = hashtags_per_tweet = retweet_ratio = 0\n",
        "\n",
        "    # New activity features\n",
        "    recent_activity = np.log1p(num_tweets)  # Frequency signal\n",
        "    url_ratio = urls_per_tweet\n",
        "    mention_ratio = mentions_per_tweet\n",
        "    retweet_ratio = retweet_ratio\n",
        "\n",
        "    features = [\n",
        "        np.log1p(followers), np.log1p(friends), np.log1p(statuses), np.log1p(favourites), np.log1p(listed),\n",
        "        fr_ratio, fl_ratio, tweets_per_day, favourites_ratio, listed_ratio,\n",
        "        verified, default_prof, default_img, geo_enabled, has_url,\n",
        "        len(desc), len(name), len(screen_name),\n",
        "        num_tweets, avg_len,\n",
        "        url_ratio, mention_ratio, hashtags_per_tweet, retweet_ratio,\n",
        "        recent_activity  # new\n",
        "    ]\n",
        "\n",
        "    return features\n",
        "\n",
        "# Prepare data\n",
        "def prepare_data_v3(data_list):\n",
        "    X, y, ids = [], [], []\n",
        "    for user in data_list:\n",
        "        X.append(extract_features_v3(user))\n",
        "        y.append(int(user.get('label', 0)))\n",
        "        ids.append(user.get('ID', ''))\n",
        "\n",
        "    X = np.nan_to_num(np.array(X, dtype=np.float32), nan=0.0, posinf=1e6, neginf=-1e6)\n",
        "    y = np.array(y, dtype=np.int64)\n",
        "    return X, y, ids\n",
        "\n",
        "# Apply\n",
        "X_train, y_train, train_ids = prepare_data_v3(train_data)\n",
        "X_test, y_test, test_ids = prepare_data_v3(test_data)\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n"
      ],
      "metadata": {
        "id": "HEs3NqrRPrbK"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import NearestNeighbors\n",
        "\n",
        "def build_graph_v3(data_list, X_features, k=20):\n",
        "    G = nx.Graph()\n",
        "    screen_to_idx = {}\n",
        "\n",
        "    # Nodes\n",
        "    for i, user in enumerate(data_list):\n",
        "        name = str(user.get('profile', {}).get('screen_name', '')).lower().strip()\n",
        "        if name:\n",
        "            screen_to_idx[name] = i\n",
        "        G.add_node(i)\n",
        "\n",
        "    edges = {'mention':0, 'knn':0}\n",
        "\n",
        "    # Mention edges\n",
        "    for i, user in enumerate(data_list):\n",
        "        tweets = user.get('tweet') or []\n",
        "        for tweet in tweets:\n",
        "            if not isinstance(tweet, str): continue\n",
        "            for word in str(tweet).split():\n",
        "                if word.startswith('@'):\n",
        "                    target = word[1:].strip('.:,;!?\\'\"()').lower()\n",
        "                    if target in screen_to_idx:\n",
        "                        j = screen_to_idx[target]\n",
        "                        if i != j:\n",
        "                            G.add_edge(i, j)\n",
        "                            edges['mention'] +=1\n",
        "\n",
        "    # K-NN behavioral similarity\n",
        "    knn = NearestNeighbors(n_neighbors=min(k+1,len(data_list)), metric='cosine')\n",
        "    knn.fit(X_features)\n",
        "    distances, indices = knn.kneighbors(X_features)\n",
        "\n",
        "    for i in range(len(data_list)):\n",
        "        for idx, j in enumerate(indices[i][1:]):  # skip self\n",
        "            if i != j and not G.has_edge(i,j):\n",
        "                sim = 1 - distances[i][idx+1]\n",
        "                if sim > 0.75:  # lower threshold => more edges\n",
        "                    G.add_edge(i,j)\n",
        "                    edges['knn'] += 1\n",
        "\n",
        "    print(f\"Graph nodes: {G.number_of_nodes()} | edges: {G.number_of_edges()}\")\n",
        "    print(f\"Mentions: {edges['mention']} | K-NN: {edges['knn']} | Avg degree: {sum(dict(G.degree()).values())/G.number_of_nodes():.2f}\")\n",
        "\n",
        "    return G\n",
        "\n",
        "# Build graphs\n",
        "G_train = build_graph_v3(train_data, X_train, k=20)\n",
        "G_test = build_graph_v3(test_data, X_test, k=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejg0UZMkxJxj",
        "outputId": "de9dd2f3-fbf9-40e8-dcf1-ff5ca50200f8"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Graph nodes: 8278 | edges: 143866\n",
            "Mentions: 91204 | K-NN: 105758 | Avg degree: 34.76\n",
            "Graph nodes: 1183 | edges: 9174\n",
            "Mentions: 3903 | K-NN: 8129 | Avg degree: 15.51\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class BotDetectorGNN_v2(torch.nn.Module):\n",
        "    def __init__(self, in_channels, hidden1=128, hidden2=64):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv(in_channels, hidden1)\n",
        "        self.conv2 = SAGEConv(hidden1, hidden2)\n",
        "        self.conv3 = SAGEConv(hidden2, 2)\n",
        "        self.dropout = 0.5\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Convert PyG format\n",
        "train_graph = nx_to_pyg(G_train, X_train, y_train).to(device)\n",
        "test_graph = nx_to_pyg(G_test, X_test, y_test).to(device)\n",
        "\n",
        "# Model\n",
        "model = BotDetectorGNN_v2(X_train.shape[1]).to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-4)\n",
        "criterion = torch.nn.NLLLoss()\n"
      ],
      "metadata": {
        "id": "OC48KNlqxMk9"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 180\n",
        "best_acc = 0\n",
        "patience = 0\n",
        "max_patience = 30  # longer patience\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    out = model(train_graph)\n",
        "    loss = criterion(out, train_graph.y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        pred_test = model(test_graph).argmax(1)\n",
        "        test_acc = (pred_test == test_graph.y).float().mean().item()\n",
        "\n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        best_state = model.state_dict()\n",
        "        patience = 0\n",
        "    else:\n",
        "        patience +=1\n",
        "\n",
        "    if (epoch+1) % 20 ==0:\n",
        "        print(f\"Epoch {epoch+1} | Loss: {loss:.4f} | Test: {test_acc*100:.2f}% | Best: {best_acc*100:.2f}%\")\n",
        "\n",
        "    if patience >= max_patience:\n",
        "        print(f\"Early stopping at epoch {epoch+1}\")\n",
        "        break\n",
        "\n",
        "model.load_state_dict(best_state)\n",
        "print(f\"✅ Best Test Accuracy: {best_acc*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m59gR7N1xPZ7",
        "outputId": "c92ef708-d674-488a-b215-5213fbc28111"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20 | Loss: 0.5118 | Test: 75.57% | Best: 75.57%\n",
            "Epoch 40 | Loss: 0.4881 | Test: 75.91% | Best: 76.08%\n",
            "Epoch 60 | Loss: 0.4724 | Test: 76.16% | Best: 76.42%\n",
            "Epoch 80 | Loss: 0.4615 | Test: 77.26% | Best: 77.43%\n",
            "Epoch 100 | Loss: 0.4568 | Test: 77.26% | Best: 77.60%\n",
            "Epoch 120 | Loss: 0.4534 | Test: 78.02% | Best: 78.02%\n",
            "Epoch 140 | Loss: 0.4445 | Test: 77.18% | Best: 78.02%\n",
            "Early stopping at epoch 150\n",
            "✅ Best Test Accuracy: 78.02%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    out = model(test_graph)\n",
        "    pred = out.argmax(1).cpu().numpy()\n",
        "    y_true = test_graph.y.cpu().numpy()\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"🎯 FINAL RESULTS\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Accuracy: {accuracy_score(y_true, pred)*100:.2f}%\")\n",
        "print(f\"Precision: {precision_score(y_true, pred)*100:.2f}%\")\n",
        "print(f\"Recall: {recall_score(y_true, pred)*100:.2f}%\")\n",
        "print(f\"F1-Score: {f1_score(y_true, pred)*100:.2f}%\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "\n",
        "print(\"📋 Detailed Classification Report:\")\n",
        "print(classification_report(y_true, pred, target_names=['Human', 'Bot']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_wgEmvZxeWS",
        "outputId": "03cdf0d6-4a72-4f08-93da-12f2673d6347"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "🎯 FINAL RESULTS\n",
            "============================================================\n",
            "Accuracy: 77.85%\n",
            "Precision: 76.32%\n",
            "Recall: 85.62%\n",
            "F1-Score: 80.71%\n",
            "============================================================\n",
            "\n",
            "📋 Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       Human       0.80      0.69      0.74       543\n",
            "         Bot       0.76      0.86      0.81       640\n",
            "\n",
            "    accuracy                           0.78      1183\n",
            "   macro avg       0.78      0.77      0.77      1183\n",
            "weighted avg       0.78      0.78      0.78      1183\n",
            "\n"
          ]
        }
      ]
    }
  ]
}